{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Submission Generator\n",
    "\n",
    "This notebook:\n",
    "- Loads trained models\n",
    "- Generates predictions on test set\n",
    "- Creates final submission file\n",
    "- Validates submission format\n",
    "\n",
    "This is the final step to generate competition-ready predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data_loading import load_processed_data\n",
    "from model_training import load_model\n",
    "from utils import create_submission\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fully engineered test data\n",
    "test = load_processed_data('../data/processed/test_with_geospatial.parquet')\n",
    "\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"Features: {test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store IDs for submission\n",
    "if 'uid' in test.columns:\n",
    "    test_ids = test['uid'].copy()\n",
    "elif 'id' in test.columns:\n",
    "    test_ids = test['id'].copy()\n",
    "else:\n",
    "    print(\"Warning: No ID column found. Creating sequential IDs.\")\n",
    "    test_ids = pd.Series(range(len(test)), name='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-feature columns\n",
    "drop_cols = ['uid', 'id', 'date', 'timestamp', 'target']\n",
    "drop_cols = [col for col in drop_cols if col in test.columns]\n",
    "\n",
    "X_test = test.drop(columns=drop_cols)\n",
    "\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"Dropped columns: {drop_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model trained on full training data\n",
    "model_full = load_model('../models/xgboost_full.pkl')\n",
    "\n",
    "# Optionally load the validated model for comparison\n",
    "model_validated = load_model('../models/xgboost_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with full model\n",
    "print(\"Generating predictions with full model...\")\n",
    "predictions_full = model_full.predict(X_test)\n",
    "\n",
    "print(f\"Predictions generated: {len(predictions_full)}\")\n",
    "print(f\"Prediction stats:\")\n",
    "print(f\"  Min: {predictions_full.min():.4f}\")\n",
    "print(f\"  Max: {predictions_full.max():.4f}\")\n",
    "print(f\"  Mean: {predictions_full.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(predictions_full):.4f}\")\n",
    "print(f\"  Std: {predictions_full.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with validated model for comparison\n",
    "print(\"\\nGenerating predictions with validated model...\")\n",
    "predictions_validated = model_validated.predict(X_test)\n",
    "\n",
    "print(f\"Prediction stats:\")\n",
    "print(f\"  Min: {predictions_validated.min():.4f}\")\n",
    "print(f\"  Max: {predictions_validated.max():.4f}\")\n",
    "print(f\"  Mean: {predictions_validated.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(predictions_validated):.4f}\")\n",
    "print(f\"  Std: {predictions_validated.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions from both models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution comparison\n",
    "axes[0].hist(predictions_full, bins=50, alpha=0.5, label='Full Model', edgecolor='black')\n",
    "axes[0].hist(predictions_validated, bins=50, alpha=0.5, label='Validated Model', edgecolor='black')\n",
    "axes[0].set_xlabel('Predicted Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Prediction Distribution Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter comparison\n",
    "axes[1].scatter(predictions_full, predictions_validated, alpha=0.3, s=10)\n",
    "axes[1].plot([predictions_full.min(), predictions_full.max()],\n",
    "             [predictions_full.min(), predictions_full.max()],\n",
    "             'r--', lw=2)\n",
    "axes[1].set_xlabel('Full Model Predictions')\n",
    "axes[1].set_ylabel('Validated Model Predictions')\n",
    "axes[1].set_title('Model Predictions Comparison')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/prediction_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.corrcoef(predictions_full, predictions_validated)[0, 1]\n",
    "print(f\"\\nCorrelation between models: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble Predictions (Optional)\n",
    "\n",
    "Can average predictions from multiple models for potentially better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble prediction (simple average)\n",
    "predictions_ensemble = (predictions_full + predictions_validated) / 2\n",
    "\n",
    "print(\"Ensemble prediction stats:\")\n",
    "print(f\"  Min: {predictions_ensemble.min():.4f}\")\n",
    "print(f\"  Max: {predictions_ensemble.max():.4f}\")\n",
    "print(f\"  Mean: {predictions_ensemble.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(predictions_ensemble):.4f}\")\n",
    "print(f\"  Std: {predictions_ensemble.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "submission_full = pd.DataFrame({\n",
    "    'uid': test_ids,\n",
    "    'target': predictions_full\n",
    "})\n",
    "\n",
    "submission_validated = pd.DataFrame({\n",
    "    'uid': test_ids,\n",
    "    'target': predictions_validated\n",
    "})\n",
    "\n",
    "submission_ensemble = pd.DataFrame({\n",
    "    'uid': test_ids,\n",
    "    'target': predictions_ensemble\n",
    "})\n",
    "\n",
    "print(\"Submission DataFrames created\")\n",
    "print(f\"Shape: {submission_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate timestamp for versioning\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save submissions\n",
    "submission_full.to_csv(f'../outputs/submissions/submission_full_{timestamp}.csv', index=False)\n",
    "submission_validated.to_csv(f'../outputs/submissions/submission_validated_{timestamp}.csv', index=False)\n",
    "submission_ensemble.to_csv(f'../outputs/submissions/submission_ensemble_{timestamp}.csv', index=False)\n",
    "\n",
    "# Also save as primary submission\n",
    "submission_full.to_csv('../outputs/submissions/submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nSubmission files saved:\")\n",
    "print(f\"  - submission_full_{timestamp}.csv\")\n",
    "print(f\"  - submission_validated_{timestamp}.csv\")\n",
    "print(f\"  - submission_ensemble_{timestamp}.csv\")\n",
    "print(f\"  - submission.csv (primary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validate Submission Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate submission\n",
    "submission_check = pd.read_csv('../outputs/submissions/submission.csv')\n",
    "\n",
    "print(\"Submission Validation:\")\n",
    "print(f\"  Shape: {submission_check.shape}\")\n",
    "print(f\"  Columns: {list(submission_check.columns)}\")\n",
    "print(f\"  Missing values: {submission_check.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicate IDs: {submission_check['uid'].duplicated().sum()}\")\n",
    "print(f\"  Data types: {submission_check.dtypes.to_dict()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(submission_check.head(10))\n",
    "\n",
    "# Check for anomalies\n",
    "if (submission_check['target'] < 0).any():\n",
    "    print(\"\\nWarning: Negative predictions detected!\")\n",
    "\n",
    "if submission_check['target'].isnull().any():\n",
    "    print(\"\\nError: Missing predictions detected!\")\n",
    "else:\n",
    "    print(\"\\nâœ“ Submission format validated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(predictions_full, bins=50, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Predicted Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Final Predictions Distribution')\n",
    "axes[0, 0].axvline(predictions_full.mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].axvline(np.median(predictions_full), color='green', linestyle='--', label='Median')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(predictions_full, vert=True)\n",
    "axes[0, 1].set_ylabel('Predicted Value')\n",
    "axes[0, 1].set_title('Predictions Box Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# QQ plot\n",
    "from scipy import stats\n",
    "stats.probplot(predictions_full, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative distribution\n",
    "sorted_preds = np.sort(predictions_full)\n",
    "cumulative = np.arange(1, len(sorted_preds) + 1) / len(sorted_preds)\n",
    "axes[1, 1].plot(sorted_preds, cumulative)\n",
    "axes[1, 1].set_xlabel('Predicted Value')\n",
    "axes[1, 1].set_ylabel('Cumulative Probability')\n",
    "axes[1, 1].set_title('Cumulative Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/final_predictions_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\"*70)\n",
    "print(\"SUBMISSION GENERATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nTest Set Size: {len(test_ids)}\")\n",
    "print(f\"Number of Features: {X_test.shape[1]}\")\n",
    "print(f\"\\nModels Used:\")\n",
    "print(f\"  1. XGBoost Full (trained on 100% data)\")\n",
    "print(f\"  2. XGBoost Validated (80/20 split)\")\n",
    "print(f\"  3. Ensemble (average of above)\")\n",
    "print(f\"\\nPrediction Statistics (Full Model):\")\n",
    "print(f\"  Count: {len(predictions_full)}\")\n",
    "print(f\"  Mean: {predictions_full.mean():.4f}\")\n",
    "print(f\"  Std: {predictions_full.std():.4f}\")\n",
    "print(f\"  Min: {predictions_full.min():.4f}\")\n",
    "print(f\"  25%: {np.percentile(predictions_full, 25):.4f}\")\n",
    "print(f\"  50%: {np.percentile(predictions_full, 50):.4f}\")\n",
    "print(f\"  75%: {np.percentile(predictions_full, 75):.4f}\")\n",
    "print(f\"  Max: {predictions_full.max():.4f}\")\n",
    "print(f\"\\nSubmission Files:\")\n",
    "print(f\"  Primary: outputs/submissions/submission.csv\")\n",
    "print(f\"  Versioned: outputs/submissions/submission_*_{timestamp}.csv\")\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"Submission generation complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "1. Loaded fully engineered test data\n",
    "2. Generated predictions using trained models\n",
    "3. Created multiple submission files:\n",
    "   - Full model (recommended)\n",
    "   - Validated model\n",
    "   - Ensemble model\n",
    "4. Validated submission format\n",
    "5. Analyzed prediction distributions\n",
    "\n",
    "**Next Steps:**\n",
    "- Submit `submission.csv` to competition\n",
    "- Monitor leaderboard performance\n",
    "- Iterate on features/models if needed\n",
    "\n",
    "**Recommended Submission:** `submission_full_{timestamp}.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
