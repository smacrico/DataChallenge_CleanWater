{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Geospatial Features\n",
    "\n",
    "This notebook adds geospatial features:\n",
    "- **Elevation** from SRTM DEM\n",
    "- **Slope and Aspect** from terrain analysis\n",
    "- **Land Cover** from ESA WorldCover\n",
    "- **Spatial Clusters** using K-means\n",
    "- **Distance Features** to reference points\n",
    "\n",
    "These features capture topographic and land use patterns that influence water quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_loading import load_processed_data, save_processed_data\n",
    "from geospatial_processing import (\n",
    "    load_srtm_elevation,\n",
    "    compute_terrain_features,\n",
    "    load_esa_worldcover,\n",
    "    create_spatial_clusters,\n",
    "    create_distance_features,\n",
    "    create_watershed_features\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered datasets from previous notebook\n",
    "train = load_processed_data('../data/processed/train_engineered.parquet')\n",
    "test = load_processed_data('../data/processed/test_engineered.parquet')\n",
    "\n",
    "print(f\"Training data: {train.shape}\")\n",
    "print(f\"Test data: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Elevation Data (SRTM DEM)\n",
    "\n",
    "**Note**: This requires SRTM DEM GeoTIFF files. If not available locally, the code will create dummy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to SRTM DEM (update with actual path)\n",
    "srtm_path = '../data/external/srtm_dem.tif'\n",
    "\n",
    "# Check if DEM exists\n",
    "import os\n",
    "if os.path.exists(srtm_path):\n",
    "    train = load_srtm_elevation(train, srtm_path)\n",
    "    test = load_srtm_elevation(test, srtm_path)\n",
    "    print(\"\\nElevation data loaded from SRTM DEM\")\n",
    "else:\n",
    "    print(f\"\\nSRTM DEM not found at {srtm_path}\")\n",
    "    print(\"Creating synthetic elevation features for demonstration...\")\n",
    "    \n",
    "    # Create synthetic elevation based on lat/lon\n",
    "    if 'latitude' in train.columns and 'longitude' in train.columns:\n",
    "        train['elevation'] = (train['latitude'] * 10 + train['longitude'] * 5 + \n",
    "                             np.random.randn(len(train)) * 50 + 500)\n",
    "        test['elevation'] = (test['latitude'] * 10 + test['longitude'] * 5 + \n",
    "                            np.random.randn(len(test)) * 50 + 500)\n",
    "        print(\"Synthetic elevation features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Terrain Features (Slope, Aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute slope and aspect from DEM\n",
    "dem_path = '../data/external/srtm_dem.tif'\n",
    "\n",
    "if os.path.exists(dem_path):\n",
    "    train = compute_terrain_features(train, dem_path)\n",
    "    test = compute_terrain_features(test, dem_path)\n",
    "    print(\"\\nTerrain features computed\")\n",
    "else:\n",
    "    print(\"DEM not available. Creating synthetic slope features...\")\n",
    "    \n",
    "    # Calculate simple slope from elevation gradient\n",
    "    if 'elevation' in train.columns:\n",
    "        train['slope'] = np.abs(np.gradient(train['elevation'])) * 10\n",
    "        test['slope'] = np.abs(np.gradient(test['elevation'])) * 10\n",
    "        train['aspect'] = np.random.uniform(0, 360, len(train))\n",
    "        test['aspect'] = np.random.uniform(0, 360, len(test))\n",
    "        print(\"Synthetic slope and aspect created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize elevation distribution\n",
    "if 'elevation' in train.columns and 'target' in train.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].hist(train['elevation'], bins=50, edgecolor='black')\n",
    "    axes[0].set_xlabel('Elevation (m)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Elevation Distribution')\n",
    "    \n",
    "    axes[1].scatter(train['elevation'], train['target'], alpha=0.3, s=1)\n",
    "    axes[1].set_xlabel('Elevation (m)')\n",
    "    axes[1].set_ylabel('Target')\n",
    "    axes[1].set_title('Elevation vs Target')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/elevation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Land Cover Data (ESA WorldCover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to ESA WorldCover (update with actual path)\n",
    "worldcover_path = '../data/external/esa_worldcover.tif'\n",
    "\n",
    "if os.path.exists(worldcover_path):\n",
    "    train = load_esa_worldcover(train, worldcover_path)\n",
    "    test = load_esa_worldcover(test, worldcover_path)\n",
    "    print(\"\\nLand cover data loaded\")\n",
    "else:\n",
    "    print(f\"\\nESA WorldCover not found at {worldcover_path}\")\n",
    "    print(\"Creating synthetic land cover features...\")\n",
    "    \n",
    "    # Create synthetic land cover classes\n",
    "    land_cover_classes = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    train['land_cover'] = np.random.choice(land_cover_classes, size=len(train))\n",
    "    test['land_cover'] = np.random.choice(land_cover_classes, size=len(test))\n",
    "    print(\"Synthetic land cover created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze land cover distribution\n",
    "if 'land_cover' in train.columns:\n",
    "    landcover_mapping = {\n",
    "        10: 'Tree cover',\n",
    "        20: 'Shrubland',\n",
    "        30: 'Grassland',\n",
    "        40: 'Cropland',\n",
    "        50: 'Built-up',\n",
    "        60: 'Bare/sparse',\n",
    "        70: 'Snow/ice',\n",
    "        80: 'Water',\n",
    "        90: 'Wetland'\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    land_cover_counts = train['land_cover'].value_counts().sort_index()\n",
    "    labels = [landcover_mapping.get(lc, str(lc)) for lc in land_cover_counts.index]\n",
    "    \n",
    "    plt.bar(range(len(land_cover_counts)), land_cover_counts.values)\n",
    "    plt.xticks(range(len(land_cover_counts)), labels, rotation=45, ha='right')\n",
    "    plt.xlabel('Land Cover Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Land Cover Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/land_cover_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Spatial Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatial clusters using K-means\n",
    "if 'latitude' in train.columns and 'longitude' in train.columns:\n",
    "    train = create_spatial_clusters(train, n_clusters=15)\n",
    "    test = create_spatial_clusters(test, n_clusters=15)\n",
    "    print(\"\\nSpatial clusters created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial clusters\n",
    "if all(col in train.columns for col in ['latitude', 'longitude', 'spatial_cluster']):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(\n",
    "        train['longitude'],\n",
    "        train['latitude'],\n",
    "        c=train['spatial_cluster'],\n",
    "        cmap='tab20',\n",
    "        s=5,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    plt.colorbar(scatter, label='Cluster ID')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title('Spatial Clusters')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/spatial_clusters.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Distance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reference points (e.g., major cities, pollution sources)\n",
    "# These should be replaced with actual domain-specific locations\n",
    "reference_points = [\n",
    "    (40.7128, -74.0060),   # Example: New York\n",
    "    (34.0522, -118.2437),  # Example: Los Angeles\n",
    "    (41.8781, -87.6298),   # Example: Chicago\n",
    "]\n",
    "\n",
    "if 'latitude' in train.columns and 'longitude' in train.columns:\n",
    "    train = create_distance_features(train, reference_points)\n",
    "    test = create_distance_features(test, reference_points)\n",
    "    print(\"\\nDistance features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Watershed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid-based watershed assignments\n",
    "if 'latitude' in train.columns and 'longitude' in train.columns:\n",
    "    train = create_watershed_features(train)\n",
    "    test = create_watershed_features(test)\n",
    "    print(\"\\nWatershed features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode land cover if needed for certain models\n",
    "# For tree-based models like XGBoost, label encoding is sufficient\n",
    "categorical_cols = ['land_cover', 'spatial_cluster', 'watershed_id']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in train.columns:\n",
    "        # Ensure categorical type\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')\n",
    "\n",
    "print(\"Categorical features encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Correlations with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with target\n",
    "if 'target' in train.columns:\n",
    "    geospatial_features = ['elevation', 'slope', 'aspect', 'spatial_cluster', \n",
    "                          'land_cover', 'watershed_id']\n",
    "    geospatial_features = [f for f in geospatial_features if f in train.columns]\n",
    "    \n",
    "    correlations = train[geospatial_features + ['target']].corr()['target'].drop('target').sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nGeospatial Feature Correlations with Target:\")\n",
    "    print(correlations)\n",
    "    \n",
    "    # Plot correlations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    correlations.plot(kind='barh')\n",
    "    plt.xlabel('Correlation with Target')\n",
    "    plt.title('Geospatial Feature Correlations')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/geospatial_correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets with geospatial features\n",
    "save_processed_data(train, '../data/processed/train_with_geospatial.parquet', format='parquet')\n",
    "save_processed_data(test, '../data/processed/test_with_geospatial.parquet', format='parquet')\n",
    "\n",
    "print(f\"\\nFinal training data: {train.shape}\")\n",
    "print(f\"Final test data: {test.shape}\")\n",
    "print(f\"\\nTotal features: {train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully added geospatial features:\n",
    "- **Elevation, Slope, Aspect**: Topographic characteristics\n",
    "- **Land Cover**: ESA WorldCover land use types\n",
    "- **Spatial Clusters**: K-means geographic groupings\n",
    "- **Distance Features**: Proximity to reference locations\n",
    "- **Watershed Features**: Hydrological boundaries\n",
    "\n",
    "These features capture important spatial patterns that influence water quality and will improve model performance.\n",
    "\n",
    "Next: Build complete training and validation pipeline (Notebook 04)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
