{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Training and Validation Pipeline\n",
    "\n",
    "This notebook implements a complete ML pipeline:\n",
    "- Loads fully engineered features\n",
    "- Performs spatial cross-validation with GroupKFold\n",
    "- Trains final XGBoost models with best parameters\n",
    "- Optional: Hyperparameter tuning with Optuna\n",
    "- Evaluates performance and saves models\n",
    "\n",
    "This is the production-ready training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data_loading import load_processed_data, split_features_target\n",
    "from model_training import (\n",
    "    BEST_XGB_PARAMS,\n",
    "    train_xgboost_model,\n",
    "    evaluate_model,\n",
    "    cross_validate_model,\n",
    "    spatial_cross_validate,\n",
    "    hyperparameter_tuning_optuna,\n",
    "    save_model,\n",
    "    get_feature_importance\n",
    ")\n",
    "from utils import save_json, print_metrics\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Fully Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets with all features\n",
    "train = load_processed_data('../data/processed/train_with_geospatial.parquet')\n",
    "test = load_processed_data('../data/processed/test_with_geospatial.parquet')\n",
    "\n",
    "print(f\"Training data: {train.shape}\")\n",
    "print(f\"Test data: {test.shape}\")\n",
    "print(f\"\\nFeature count: {train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns to drop\n",
    "id_cols = ['uid', 'id'] if 'uid' in train.columns or 'id' in train.columns else []\n",
    "date_cols = ['date', 'timestamp'] if 'date' in train.columns or 'timestamp' in train.columns else []\n",
    "drop_cols = list(set(id_cols + date_cols))\n",
    "\n",
    "# Remove columns that don't exist\n",
    "drop_cols = [col for col in drop_cols if col in train.columns]\n",
    "\n",
    "print(f\"Dropping columns: {drop_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X_full, y_full = split_features_target(train, target_col='target', drop_cols=drop_cols)\n",
    "X_test, _ = split_features_target(test, target_col='target', drop_cols=drop_cols)\n",
    "\n",
    "print(f\"\\nFeature matrix: {X_full.shape}\")\n",
    "print(f\"Target: {y_full.shape}\")\n",
    "print(f\"Test features: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure test has same columns as train\n",
    "missing_cols = set(X_full.columns) - set(X_test.columns)\n",
    "extra_cols = set(X_test.columns) - set(X_full.columns)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Adding missing columns to test: {missing_cols}\")\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"Removing extra columns from test: {extra_cols}\")\n",
    "    X_test = X_test.drop(columns=extra_cols)\n",
    "\n",
    "# Reorder columns\n",
    "X_test = X_test[X_full.columns]\n",
    "print(f\"\\nAligned test features: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full,\n",
    "    y_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "cv_results = cross_validate_model(\n",
    "    X_full,\n",
    "    y_full,\n",
    "    params=BEST_XGB_PARAMS,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Save CV results\n",
    "save_json(cv_results, '../outputs/logs/cv_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spatial Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spatial clusters for spatial CV\n",
    "if 'spatial_cluster' in train.columns:\n",
    "    spatial_groups = train['spatial_cluster']\n",
    "    \n",
    "    spatial_cv_results = spatial_cross_validate(\n",
    "        X_full,\n",
    "        y_full,\n",
    "        spatial_groups=spatial_groups,\n",
    "        params=BEST_XGB_PARAMS,\n",
    "        n_splits=5\n",
    "    )\n",
    "    \n",
    "    # Save spatial CV results\n",
    "    save_json(spatial_cv_results, '../outputs/logs/spatial_cv_results.json')\n",
    "else:\n",
    "    print(\"Spatial cluster column not found. Skipping spatial CV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optional: Hyperparameter Tuning with Optuna\n",
    "\n",
    "**Note**: This section is optional and can be skipped if using BEST_XGB_PARAMS.\n",
    "Uncomment to run hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to run Optuna hyperparameter tuning\n",
    "# RUN_OPTUNA = False  # Set to True to enable tuning\n",
    "\n",
    "# if RUN_OPTUNA:\n",
    "#     print(\"Starting Optuna hyperparameter tuning...\")\n",
    "#     tuning_results = hyperparameter_tuning_optuna(\n",
    "#         X_train,\n",
    "#         y_train,\n",
    "#         X_val,\n",
    "#         y_val,\n",
    "#         n_trials=50,\n",
    "#         timeout=3600  # 1 hour\n",
    "#     )\n",
    "#     \n",
    "#     # Save tuned parameters\n",
    "#     save_json(tuning_results['best_params'], '../outputs/logs/optuna_best_params.json')\n",
    "#     \n",
    "#     # Use tuned parameters\n",
    "#     FINAL_PARAMS = tuning_results['best_params']\n",
    "#     FINAL_PARAMS['n_estimators'] = 900\n",
    "#     FINAL_PARAMS['objective'] = 'reg:squarederror'\n",
    "#     FINAL_PARAMS['tree_method'] = 'hist'\n",
    "#     FINAL_PARAMS['random_state'] = 42\n",
    "#     FINAL_PARAMS['n_jobs'] = -1\n",
    "# else:\n",
    "#     FINAL_PARAMS = BEST_XGB_PARAMS\n",
    "\n",
    "# Use best parameters\n",
    "FINAL_PARAMS = BEST_XGB_PARAMS\n",
    "print(\"Using BEST_XGB_PARAMS for final model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full training set\n",
    "print(\"Training final XGBoost model...\")\n",
    "final_model = train_xgboost_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    params=FINAL_PARAMS,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "train_metrics = evaluate_model(final_model, X_train, y_train, \"Training Set\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_metrics = evaluate_model(final_model, X_val, y_val, \"Validation Set\")\n",
    "\n",
    "# Save metrics\n",
    "all_metrics = {\n",
    "    'train': train_metrics,\n",
    "    'validation': val_metrics,\n",
    "    'cv': cv_results\n",
    "}\n",
    "save_json(all_metrics, '../outputs/logs/final_model_metrics.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Predictions vs Actuals Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "y_val_pred = final_model.predict(X_val)\n",
    "\n",
    "# Plot predictions vs actuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.3, s=10)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual')\n",
    "axes[0].set_ylabel('Predicted')\n",
    "axes[0].set_title(f'Training Set (R²={train_metrics[\"r2\"]:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.3, s=10)\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual')\n",
    "axes[1].set_ylabel('Predicted')\n",
    "axes[1].set_title(f'Validation Set (R²={val_metrics[\"r2\"]:.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/predictions_vs_actuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance_df = get_feature_importance(\n",
    "    final_model,\n",
    "    X_train.columns.tolist(),\n",
    "    top_n=30\n",
    ")\n",
    "\n",
    "print(\"\\nTop 30 Most Important Features:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(range(len(importance_df)), importance_df['importance'])\n",
    "plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 30 Feature Importances - Final Model')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/final_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save importance to CSV\n",
    "importance_df.to_csv('../outputs/logs/feature_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Residuals Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_val - y_val_pred\n",
    "\n",
    "# Plot residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals distribution\n",
    "axes[0].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[0].set_xlabel('Residual')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Residuals Distribution')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Residuals vs predicted\n",
    "axes[1].scatter(y_val_pred, residuals, alpha=0.3, s=10)\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Value')\n",
    "axes[1].set_ylabel('Residual')\n",
    "axes[1].set_title('Residuals vs Predicted')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model\n",
    "save_model(final_model, '../models/xgboost_final.pkl')\n",
    "print(\"\\nFinal model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train Models on Full Dataset (Optional)\n",
    "\n",
    "Train on 100% of training data for final submission predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full dataset without validation split\n",
    "print(\"Training model on 100% of training data...\")\n",
    "\n",
    "full_model = train_xgboost_model(\n",
    "    X_full,\n",
    "    y_full,\n",
    "    X_val=None,\n",
    "    y_val=None,\n",
    "    params=FINAL_PARAMS,\n",
    "    early_stopping_rounds=None,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Save full model\n",
    "save_model(full_model, '../models/xgboost_full.pkl')\n",
    "print(\"Full dataset model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented a complete training and validation pipeline:\n",
    "\n",
    "### Models Trained:\n",
    "1. **xgboost_final.pkl**: Trained with 80/20 split, validated\n",
    "2. **xgboost_full.pkl**: Trained on 100% of data for submission\n",
    "\n",
    "### Validation Strategies:\n",
    "- Standard K-Fold Cross-Validation\n",
    "- Spatial GroupKFold Cross-Validation\n",
    "- Train/Validation holdout split\n",
    "\n",
    "### Performance:\n",
    "- Training R²: {train_metrics['r2']:.4f}\n",
    "- Validation R²: {val_metrics['r2']:.4f}\n",
    "- CV Mean R²: {cv_results['mean_r2']:.4f}\n",
    "\n",
    "Next: Generate submission predictions (Notebook 05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
